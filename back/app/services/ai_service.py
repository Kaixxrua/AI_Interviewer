# app/services/ai_service.py

import os
import json
import re
import datetime
import random  #
from typing import Iterator, Optional, Dict, Any

from google import genai
from google.genai import types
from dotenv import load_dotenv
from sqlalchemy.orm import Session
from sqlalchemy import desc
from app.model import ChatMessage

# å°è¯•å¯¼å…¥ RAG æœåŠ¡
try:
    from app.services.rag_service import search_knowledge
except ImportError:
    print("âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ° app.services.rag_serviceï¼ŒRAG åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")

    def search_knowledge(*args, **kwargs):
        return []


TOPIC_VARIATIONS = {
    "Python": [
        "æ·±æ‹·è´ä¸æµ…æ‹·è´ (Deep vs Shallow Copy)",
        "è£…é¥°å™¨ (Decorators) çš„åŸç†ä¸åº”ç”¨",
        "ç”Ÿæˆå™¨ (Generators) ä¸è¿­ä»£å™¨ (Iterators)",
        "Python çš„å†…å­˜ç®¡ç†ä¸åƒåœ¾å›æ”¶æœºåˆ¶",
        "å¯å˜å¯¹è±¡ä¸ä¸å¯å˜å¯¹è±¡ (Mutable vs Immutable)",
        "å­—å…¸ (Dict) çš„åº•å±‚å®ç°åŸç†",
        "GIL (å…¨å±€è§£é‡Šå™¨é”) å¯¹å¤šçº¿ç¨‹çš„å½±å“",
        "é—­åŒ… (Closure) ä¸ä½œç”¨åŸŸ",
        "å¼‚å¸¸å¤„ç†æœºåˆ¶ (try-except-else-finally)",
        "é­”æ³•æ–¹æ³• (Magic Methods) å¦‚ __init__, __new__, __call__",
    ],
    "Frontend": [  # å‰ç«¯
        "Vue/React çš„ç”Ÿå‘½å‘¨æœŸ",
        "æµè§ˆå™¨æ¸²æŸ“åŸç†ä¸é‡ç»˜é‡æ’",
        "JS åŸå‹é“¾ä¸ç»§æ‰¿",
        "ES6 æ–°ç‰¹æ€§ (Promise, Async/Await)",
        "è·¨åŸŸé—®é¢˜çš„è§£å†³æ–¹æ¡ˆ",
        "å‰ç«¯æ€§èƒ½ä¼˜åŒ–æ‰‹æ®µ",
        "CSS ç›’æ¨¡å‹ä¸ BFC",
        "Vue å“åº”å¼åŸç† (Object.defineProperty vs Proxy)",
    ],
    # ... ä½ å¯ä»¥ç»§ç»­æ‰©å±•å…¶ä»–åˆ†ç±»
}

# å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œå°±ç”¨è¿™ä¸ªé€šç”¨çš„
GENERIC_VARIATIONS = [
    "è¯¥æŠ€æœ¯æ ˆçš„åº•å±‚åŸç†",
    "å®é™…å¼€å‘ä¸­å®¹æ˜“è¸©çš„å‘",
    "æ€§èƒ½ä¼˜åŒ–çš„æœ€ä½³å®è·µ",
    "ä¸åŒç±»æŠ€æœ¯çš„å¯¹æ¯”ä¼˜åŠ¿",
    "æ ¸å¿ƒæ¶æ„è®¾è®¡æ€æƒ³",
]
load_dotenv()

# é…ç½®ä»£ç†
os.environ["HTTP_PROXY"] = "http://127.0.0.1:7895"
os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7895"

api_key = os.getenv("API_KEY")
if not api_key:
    print("âŒ æœªæ‰¾åˆ° API_KEY")

try:
    client = genai.Client(api_key=api_key)
except Exception as e:
    client = None
    print(f"âŒ Gemini Client åˆå§‹åŒ–å¤±è´¥: {e}")


def get_system_prompt(use_deep_thinking: bool = False, interview_context: dict = None):
    current_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    base_prompt = f"å½“å‰æ—¶é—´ï¼š{current_time}\n"

    if interview_context:
        topic = interview_context.get("topic", "é€šç”¨æŠ€æœ¯")  # ä¾‹å¦‚ "Pythonå¼€å‘"
        difficulty = interview_context.get(
            "difficulty", "ä¸­çº§"
        )  # ä¾‹å¦‚ "[æ ¡æ‹›/å®ä¹ ] - è¯­æ³•åŸºç¡€"
        current_round = interview_context.get("current_round", 1)
        max_rounds = interview_context.get("max_rounds", 10)

        # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šç¬¬ä¸€è½®å¼ºåˆ¶éšæœºé€‰é¢˜ ğŸ”¥ğŸ”¥ğŸ”¥
        random_instruction = ""
        if current_round == 1:
            # 1. ç®€å•çš„å…³é”®è¯åŒ¹é…ï¼Œç¡®å®šæŠ€æœ¯æ ˆ
            key = (
                "Python"
                if "Python" in topic or "Python" in difficulty
                else "Frontend" if "å‰ç«¯" in topic or "Vue" in topic else None
            )

            # 2. éšæœºæŠ½å–ä¸€ä¸ªä¾§é‡ç‚¹
            if key and key in TOPIC_VARIATIONS:
                focus_point = random.choice(TOPIC_VARIATIONS[key])
            else:
                focus_point = random.choice(GENERIC_VARIATIONS)

            # 3. æ„é€ å¼ºåˆ¶æŒ‡ä»¤
            random_instruction = f"""
            ã€ç‰¹åˆ«æŒ‡ä»¤ã€‘ï¼šè¿™æ˜¯é¢è¯•çš„ç¬¬ä¸€é¢˜ã€‚
            ä¸ºäº†é¿å…é‡å¤ï¼Œè¯·ä½ **å¿…é¡»**å¿½ç•¥â€œåˆ—è¡¨ä¸å…ƒç»„çš„åŒºåˆ«â€è¿™ç§è€å¥—é—®é¢˜ã€‚
            è¯·é‡ç‚¹è€ƒå¯Ÿï¼šã€{focus_point}ã€‘ã€‚
            è¯·ç»“åˆå€™é€‰äººçš„çº§åˆ«ï¼ˆ{difficulty}ï¼‰è®¾è®¡ä¸€ä¸ªå…·ä½“çš„é—®é¢˜ã€‚
            """

        base_prompt += f"""
ä½ ç°åœ¨æ˜¯ä¸€ä½èµ„æ·±çš„ã€{topic}ã€‘é¢è¯•å®˜ï¼Œæ­£åœ¨é¢è¯•ä¸€ä½ã€{difficulty}ã€‘æ°´å¹³çš„å€™é€‰äººã€‚
å½“å‰é¢è¯•è¿›åº¦ï¼šç¬¬ {current_round} / {max_rounds} è½®ã€‚

ã€ä½ çš„è¡Œä¸ºå‡†åˆ™ã€‘ï¼š
1. ä¸¥ç¦ä¸€æ¬¡æ€§é—®å¤šä¸ªé—®é¢˜ï¼Œæ¯æ¬¡åªé—®ä¸€ä¸ªã€‚
2. å¦‚æœæ˜¯ç¬¬ 1 è½®ï¼Œè¯·ç›´æ¥å¼€å§‹æé—®ï¼Œä¸è¦å¯’æš„ã€‚
{random_instruction} 
3. å¦‚æœç”¨æˆ·å›ç­”äº†ä¸Šä¸€ä¸ªé—®é¢˜ï¼Œè¯·ç®€çŸ­ç‚¹è¯„ï¼Œç„¶åç´§æ¥ç€é—®ä¸‹ä¸€ä¸ªé—®é¢˜ã€‚
4. ä¿æŒä¸“ä¸šï¼Œé—®é¢˜è¦å…·ä½“ï¼Œä¸è¦å¤ªæ³›ã€‚
"""
    else:
        base_prompt += "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI é¢è¯•å®˜å’ŒæŠ€æœ¯åŠ©æ‰‹ã€‚\n"

    if use_deep_thinking:
        base_prompt += """
\nã€æ ¼å¼å¼ºåˆ¶æŒ‡ä»¤ã€‘
ä½ ç°åœ¨å¤„äºã€æ·±åº¦æ€è€ƒæ¨¡å¼ã€‘ã€‚
è¯·åŠ¡å¿…å°†ä½ çš„æ€è€ƒè¿‡ç¨‹åŒ…è£¹åœ¨ <think> å’Œ </think> æ ‡ç­¾ä¸­ã€‚
å…ˆè¾“å‡º <think>...æ€è€ƒå†…å®¹...</think>ï¼Œå†è¾“å‡ºæœ€ç»ˆå›ç­”ã€‚
"""
    return base_prompt


def upload_file_to_gemini(file_path: str, mime_type: str = None):
    if not client:
        return None
    try:
        uploaded_file = client.files.upload(
            file=file_path, config={"mime_type": mime_type} if mime_type else None
        )
        return uploaded_file
    except Exception:
        return None


def save_message(
    db: Session,
    session_id: str,
    role: str,
    content: str,
    image_url: str = None,
    file_uri: str = None,
    file_mime_type: str = None,
    file_original_name: str = None,
):
    new_msg = ChatMessage(
        session_id=session_id,
        role=role,
        content=content,
        image_url=image_url,
        file_uri=file_uri,
        file_mime_type=file_mime_type,
        file_original_name=file_original_name,
    )
    db.add(new_msg)
    db.commit()
    db.refresh(new_msg)
    return new_msg.id


def get_chat_history(db: Session, session_id: str, limit: int = 20):
    messages = (
        db.query(ChatMessage)
        .filter(ChatMessage.session_id == session_id)
        .order_by(desc(ChatMessage.created_at))
        .limit(limit)
        .all()
    )
    messages.reverse()

    formatted_history = []
    for msg in messages:
        parts = []
        if msg.file_uri and msg.file_mime_type:
            parts.append(
                types.Part.from_uri(file_uri=msg.file_uri, mime_type=msg.file_mime_type)
            )

        clean_content = msg.content
        if "<think>" in clean_content:
            import re

            clean_content = re.sub(
                r"<think>[\s\S]*?</think>", "", clean_content
            ).strip()

        parts.append(types.Part.from_text(text=clean_content))
        formatted_history.append(types.Content(role=msg.role, parts=parts))
    return formatted_history


def stream_ai_response(
    db: Session,
    session_id: str,
    user_message: str,
    use_deep_thinking: bool = False,
    use_search: bool = False,
    memory_limit: int = 10,
    pre_uploaded_file_uri: Optional[str] = None,
    file_mime_type: Optional[str] = None,
    file_original_name: Optional[str] = None,
    use_rag_bank: bool = False,
    interview_context: Optional[dict] = None,
) -> Iterator[str]:

    if not client:
        yield "ç³»ç»Ÿé”™è¯¯ï¼šAPI Key æœªé…ç½®"
        return

    full_response_text = ""
    gemini_file_uri = pre_uploaded_file_uri

    try:
        # 1. è·å–å†å²
        msg_limit = memory_limit * 2
        history_contents = []
        if msg_limit > 0:
            history_contents = get_chat_history(db, session_id, limit=msg_limit)

        # 2. RAG æ£€ç´¢ (å·²å¢åŠ  Prompt éš”ç¦»)
        rag_context = ""
        if use_rag_bank:
            try:
                found_docs = search_knowledge(user_message)
                if found_docs:
                    if isinstance(found_docs, list):
                        doc_text = "\n\n".join(found_docs)
                    else:
                        doc_text = str(found_docs)
                    current_topic = (interview_context or {}).get("topic", "é€šç”¨æŠ€æœ¯")
                    # ğŸ”¥ğŸ”¥ğŸ”¥ æ ¸å¿ƒä¿®æ”¹ï¼šç”©é”…å¼ Prompt ğŸ”¥ğŸ”¥ğŸ”¥
                    rag_context = f"""
ã€âš ï¸ç³»ç»Ÿå†…éƒ¨å‚è€ƒèµ„æ–™ã€‘
(æ³¨æ„ï¼šä»¥ä¸‹å†…å®¹æ£€ç´¢è‡ªæœ¬åœ°æ•°æ®åº“ï¼Œå¯èƒ½åŒ…å«ä¸å½“å‰é¢è¯•ä¸»é¢˜({current_topic})æ— å…³çš„ä»£ç ã€‚
å¦‚æœå‚è€ƒèµ„æ–™è¯­è¨€ä¸ç¬¦ï¼Œè¯·åŠ¡å¿…ç›´æ¥å¿½ç•¥ï¼Œä¸¥ç¦åœ¨å›å¤ä¸­å¼•ç”¨ï¼Œæ›´ä¸è¦è®¤ä¸ºæ˜¯ç”¨æˆ·æä¾›çš„ï¼)

---èµ„æ–™å¼€å§‹---
{doc_text}
---èµ„æ–™ç»“æŸ---
"""
                    print(f"âœ… å‘½ä¸­çŸ¥è¯†åº“: {len(found_docs)} æ¡ (å·²è¿‡æ»¤ä¸ç›¸å…³å†…å®¹)")
            except Exception as e:
                print(f"âŒ RAG Error: {e}")

        # 3. æ„å»º Prompt
        final_prompt = user_message
        if rag_context:
            final_prompt = f"{rag_context}\nç”¨æˆ·é—®é¢˜ï¼š{user_message}"

        if file_mime_type == "application/pdf":
            final_prompt = f"è¯·é˜…è¯»é™„ä»¶ PDFï¼š{file_original_name}ã€‚\n{final_prompt}"

        current_parts = []
        if gemini_file_uri:
            current_parts.append(
                types.Part.from_uri(file_uri=gemini_file_uri, mime_type=file_mime_type)
            )

        current_parts.append(types.Part.from_text(text=final_prompt))
        final_contents = history_contents + [
            types.Content(role="user", parts=current_parts)
        ]

        tools = []
        if use_search:
            tools.append(types.Tool(google_search=types.GoogleSearch()))

        # ğŸ”¥ ä¿æŒä½ çš„æ¨¡å‹åç§° ğŸ”¥
        if use_deep_thinking:
            current_model = "gemini-3-pro-preview"
        else:
            current_model = "gemini-3-flash-preview"

        print(f"ğŸš€ Model: {current_model}, DeepThinking: {use_deep_thinking}")

        response_stream = client.models.generate_content_stream(
            model=current_model,
            contents=final_contents,
            config=types.GenerateContentConfig(
                system_instruction=get_system_prompt(
                    use_deep_thinking, interview_context
                ),
                temperature=0.7,
                tools=tools,
                max_output_tokens=8192,
            ),
        )

        for chunk in response_stream:
            text_chunk = chunk.text if hasattr(chunk, "text") else ""
            if text_chunk:
                full_response_text += text_chunk
                yield text_chunk

    except Exception as e:
        print(f"Stream Error: {e}")
        yield f"\n[æœåŠ¡å¼‚å¸¸: {str(e)}]"


async def generate_interview_report(chat_history: list) -> Dict[str, Any]:
    """
    æ ¹æ®å¯¹è¯å†å²ç”Ÿæˆç»“æ„åŒ–æŠ¥å‘Š (åŸºäº WHWS å››ç»´æ¨¡å‹è¯„åˆ†)
    """
    if not client:
        return {
            "score": 0,
            "comment": "AI æœåŠ¡æœªè¿æ¥",
            "strengths": [],
            "suggestions": [],
        }

    # 1. è½¬æ¢å¯¹è¯å†å²
    history_text = ""
    for msg in chat_history:
        role = "é¢è¯•å®˜" if msg.get("role") == "model" else "å€™é€‰äºº"
        history_text += f"{role}: {msg.get('content')}\n"

    # 2. æ„é€ æ·±åº¦è¯„ä¼° Prompt (èå…¥ä½ çš„è¯„åˆ†å“²å­¦)
    system_prompt = """
    ä½ æ˜¯ä¸€ä½æåº¦ä¸¥æ ¼ä¸”ä¸“ä¸šçš„èµ„æ·±æŠ€æœ¯é¢è¯•å®˜ã€‚é¢è¯•å·²ç»“æŸï¼Œè¯·æ ¹æ®ã€å¯¹è¯å†å²ã€‘ç”Ÿæˆä¸€ä»½è¯„ä¼°æŠ¥å‘Šã€‚
    
    ã€æ ¸å¿ƒè¯„åˆ†æ ‡å‡†ã€‘ï¼š
    æŠ€æœ¯é¢è¯•ä¸æ˜¯â€œé—®ç­”è€ƒè¯•â€ï¼Œè€Œæ˜¯â€œåŒè¡Œåˆ‡ç£‹â€ã€‚
    è¯·ä¸¥æ ¼æŒ‰ç…§ **What-How-Why-Scenarios (WHWS)** å››ç»´æ¨¡å‹å¯¹å€™é€‰äººçš„å›ç­”è¿›è¡Œæ‹†è§£è¯„åˆ†ï¼š
    
    1. **What (å®šä¹‰/ç»“è®º)** - æƒé‡ 20%
       - æ˜¯å¦å‡†ç¡®ç»™å‡ºäº†å®šä¹‰ï¼Ÿ
       - å¦‚æœåªå›ç­”äº†è¿™ä¸€å±‚ï¼Œè§†ä¸ºâ€œæŒ¤ç‰™è†â€å¼å›ç­”ï¼Œåªèƒ½ç»™ä½åˆ†ã€‚
       
    2. **How (åŸç†/æºç )** - æƒé‡ 30%
       - æ˜¯å¦å±•ç¤ºäº†æ·±åº¦ï¼Ÿ(å¦‚ï¼šåº•å±‚æ•°æ®ç»“æ„ã€æºç æœºåˆ¶ã€çº¿ç¨‹å®‰å…¨å®ç°ç­‰)ã€‚
       - è¿™æ˜¯åŒºåˆ«åˆçº§å’Œé«˜çº§çš„å…³é”®ã€‚
       
    3. **Why & Comparison (å¯¹æ¯”/é€‰å‹)** - æƒé‡ 20%
       - æ˜¯å¦å±•ç¤ºäº†å¹¿åº¦ï¼Ÿ(å¦‚ï¼šä¸å…¶ä»–æŠ€æœ¯æ–¹æ¡ˆçš„æ¨ªå‘å¯¹æ¯”ã€ä¼˜ç¼ºç‚¹æƒè¡¡)ã€‚
       
    4. **Scenarios (åœºæ™¯/å®æˆ˜)** - æƒé‡ 30% (æ€æ‰‹é”)
       - æ˜¯å¦ç»“åˆäº†å®é™…é¡¹ç›®ç»éªŒï¼Ÿ
       - æ˜¯å¦æåˆ°äº†å…·ä½“çš„ç”Ÿäº§é—®é¢˜ï¼ˆå¦‚OOMã€æ­»é”ã€æ€§èƒ½ä¼˜åŒ–ï¼‰åŠè§£å†³æ–¹æ¡ˆï¼Ÿ
    
    ã€åˆ†æ•°æ®µä½å®šä¹‰ã€‘(è¯·ä¸¥æ ¼æ‰§è¡Œ):
    - **< 60åˆ† (ä¸é€šè¿‡)**: å›ç­”å¹²ç˜ªï¼Œä»…åœç•™åœ¨ What å±‚é¢ï¼ŒåƒæŒ¤ç‰™è†ä¸€æ ·ï¼Œä¸€é—®ä¸€ç­”ï¼Œç¼ºä¹æ·±åº¦ã€‚
    - **60 - 84åˆ† (é€šè¿‡)**: åŸºç¡€æ‰å®ï¼Œèƒ½å›ç­”å‡º What å’Œ Howï¼Œå¯¹åŸç†æœ‰ä¸€å®šç†è§£ï¼Œä½†ç¼ºä¹å®æˆ˜åœºæ™¯çš„ç»“åˆã€‚
    - **â‰¥ 85åˆ† (è–ªèµ„è°ˆåˆ¤çº§)**: å®Œç¾æŒæ¡ WHWS æ¨¡å‹ã€‚ä¸ä»…æ‡‚åŸç†ï¼Œè¿˜èƒ½æ¨ªå‘å¯¹æ¯”(Why)ï¼Œå¹¶èƒ½ä¸»åŠ¨åˆ†äº«å®æˆ˜è¸©å‘ç»éªŒ(Scenarios)ï¼Œä¸»å¯¼å¯¹è¯èŠ‚å¥ã€‚

    ã€ç‰¹æ®Šæƒ…å†µå¤„ç†ã€‘ï¼š
    å¦‚æœã€å¯¹è¯å†å²ã€‘éå¸¸çŸ­ï¼ˆä¾‹å¦‚åªæœ‰ 1-2 è½®é—®ç­”ï¼‰ï¼Œä¸”å€™é€‰äººä¸»åŠ¨ç»“æŸæˆ–å›ç­”å¾ˆå°‘ï¼š
    1. åˆ†æ•°è¯·ç»™å‡º **5-15åˆ†** ä¹‹é—´çš„â€œå‚ä¸åˆ†â€ã€‚
    2. **è¯„è¯­å¿…é¡»åŒ…å«**ï¼šâ€œç”±äºé¢è¯•è½®æ¬¡è¿‡å°‘ï¼Œæ— æ³•å…¨é¢è¯„ä¼°...â€ å­—æ ·ã€‚
    3. è¿™ç§æƒ…å†µä¸‹ï¼ŒStrengths å¯ä»¥å†™â€œå°šæ— è¶³å¤Ÿæ•°æ®è¯„ä¼°â€ï¼ŒSuggestions å¯ä»¥å»ºè®®â€œè¯·å°è¯•å®Œæˆå®Œæ•´çš„ 10 è½®é¢è¯•â€ã€‚

    ã€è¾“å‡ºæ ¼å¼è¦æ±‚ã€‘ï¼š
    1. å¿…é¡»è¿”å›çº¯ JSON æ ¼å¼ã€‚
    2. ä¸è¦ä½¿ç”¨ Markdown æ ‡è®°ã€‚
    3. ç»“æ„å¦‚ä¸‹ï¼š
    {
        "score": (0-100 æ•´æ•°),
        "comment": "ç®€çŸ­çŠ€åˆ©çš„ç»¼åˆè¯„ä»·ï¼Œè¯·æŒ‡å‡ºä»–å¤„äºå“ªä¸ªå±‚çº§ï¼ˆåˆçº§/è¿›é˜¶/ä¸“å®¶ï¼‰",
        "strengths": ["æ ¹æ®WHWSæ¨¡å‹å‘ç°çš„äº®ç‚¹1", "äº®ç‚¹2"],
        "suggestions": ["é’ˆå¯¹ç¼ºå¤±ç»´åº¦çš„å…·ä½“å»ºè®®1", "å»ºè®®2"]
    }
    """

    full_prompt = f"{system_prompt}\n\nã€å¯¹è¯å†å²ã€‘:\n{history_text}"

    try:
        response = client.models.generate_content(
            model="gemini-3-pro-preview",
            contents=full_prompt,
            config=types.GenerateContentConfig(
                temperature=0.3,  # é™ä½æ¸©åº¦ï¼Œè®©è¯„åˆ†æ›´å®¢è§‚ç¨³å®š
                response_mime_type="application/json",
                max_output_tokens=65535,
            ),
        )

        raw_text = response.text
        clean_json = raw_text.replace("```json", "").replace("```", "").strip()
        data = json.loads(clean_json)

        return {
            "score": data.get("score", 60),
            "comment": data.get("comment", "æš‚æ— è¯„ä»·"),
            "strengths": data.get("strengths", []),
            "suggestions": data.get("suggestions", []),
        }

    except Exception as e:
        print(f"Report Generation Error: {e}")
        return {
            "score": 0,
            "comment": "AI é˜…å·æœåŠ¡ç¹å¿™ï¼Œè¯·ç¨åé‡è¯•",
            "strengths": [],
            "suggestions": [],
        }


def get_llm_response(messages: list) -> str:
    """
    éæµå¼ç®€å•è°ƒç”¨ï¼Œä¾›è„šæœ¬æˆ–åå°ä»»åŠ¡ä½¿ç”¨
    :param messages: [{"role": "user", "content": "..."}]
    :return: AI çš„å®Œæ•´æ–‡æœ¬å›å¤
    """
    if not client:
        return "Error: Gemini Client not initialized"

    try:
        # å–å‡ºæœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯çš„å†…å®¹
        prompt = messages[-1]["content"]

        # è¿™é‡Œçš„ messages æ ¼å¼å¯èƒ½åŒ…å« system promptï¼Œä½†åœ¨ç®€å•è„šæœ¬é‡Œæˆ‘ä»¬ç®€åŒ–å¤„ç†
        # ç›´æ¥è°ƒç”¨ç”Ÿæˆå†…å®¹
        response = client.models.generate_content(
            model="gemini-3-flash-preview",  # ç”¨ Flash æ¨¡å‹æ¯”è¾ƒå¿«ä¸”ä¾¿å®œ
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=0.7, max_output_tokens=65535
            ),
        )

        return response.text if response.text else ""

    except Exception as e:
        print(f"LLM Call Error: {e}")
        return ""
